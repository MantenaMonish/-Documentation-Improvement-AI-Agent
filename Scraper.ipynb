{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa843d69",
   "metadata": {},
   "source": [
    "## Documentation Scraping Agent\n",
    "\n",
    "### Objective\n",
    "This component extracts technical documentation content from MoEngage's developer portal while bypassing potential anti-scraping measures. The extracted content is converted to clean Markdown format for downstream analysis.\n",
    "\n",
    "### Technical Approach\n",
    "#### Core Technologies\n",
    "- **Bright Data Residential Proxies**: To bypass IP restrictions and avoid blocking\n",
    "- **Requests**: For HTTP communication with retry mechanism\n",
    "- **BeautifulSoup**: HTML parsing and content extraction\n",
    "- **html2text**: Conversion of HTML to Markdown format\n",
    "- **Regex**: Content cleanup and filename sanitization\n",
    "\n",
    "#### Key Features\n",
    "1. **Proxy Integration**:\n",
    "   - Uses Bright Data residential proxies to mimic real user traffic\n",
    "   - Configurable via environment variables for security\n",
    "   - Automatic retry mechanism (3 attempts) for network resilience\n",
    "\n",
    "2. **Content Extraction**:\n",
    "   - Targets specific CSS classes (`article__body markdown`) for main content\n",
    "   - Multiple fallback strategies if primary content container not found\n",
    "   - Cleans unnecessary elements (scripts, styles, footers, etc.)\n",
    "\n",
    "3. **Markdown Conversion**:\n",
    "   - Preserves links and basic formatting\n",
    "   - Skips images to reduce token usage in later stages\n",
    "   - Aggressive cleanup of whitespace and line breaks\n",
    "\n",
    "4. **File Management**:\n",
    "   - Automatic directory creation (`scraped_docs`)\n",
    "   - Filename generation from article titles\n",
    "   - Special character sanitization for compatibility\n",
    "\n",
    "### Why This Approach?\n",
    "- **Anti-Scraping Mitigation**: Residential proxies help avoid IP bans that are common with technical documentation portals\n",
    "- **Content Focus**: Targets only relevant technical content, ignoring navigation and site chrome\n",
    "- **Format Preservation**: Maintains code blocks and technical formatting crucial for SDK documentation\n",
    "- **Error Resilience**: Comprehensive exception handling for network and parsing issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8905d71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing: https://developers.moengage.com/hc/en-us/articles/360061108111-Web-SDK-Overview#h_01H9G1YMFWVN61PKBDN0MGAJWG\n",
      "==================================================\n",
      "Extracted content saved to: scraped_docs\\web_sdk_overview.md\n",
      "Title: Web SDK Overview\n",
      "Content length: 1679 characters\n",
      "Content preview:\n",
      "# Introduction to Web Modules The following modules are supported for Web SDK Integration: ## Web Push Permission based messages are sent regardless of whether the user is on your website or not. ## Analytics Any data collection on your users for your brand use. Typical use cases are to personalize ...\n",
      "\n",
      "\n",
      "==================================================\n",
      "Processing: https://developers.moengage.com/hc/en-us/articles/22105190881044-Getting-Started-with-React-Native-SDK#h_01HEJAHP5W49AASNSHF5614AHP\n",
      "==================================================\n",
      "Extracted content saved to: scraped_docs\\getting_started_with_react_native_sdk.md\n",
      "Title: Getting Started with React Native SDK\n",
      "Content length: 16102 characters\n",
      "Content preview:\n",
      "# Overview MoEngageâ€™s React Native SDK helps you integrate MoEngage into iOS and Android applications built with React Native framework. It allows you to work with push notifications, in-app messages, cards, user attributes, events, and much more. To see the sample code, take a look at the [GitHub r...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import re\n",
    "import os\n",
    "import urllib3\n",
    "from urllib.parse import urlparse\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Configure your Bright Data residential proxy here\n",
    "PROXY_HOST = os.getenv('PROXY_HOST', 'brd.superproxy.io')\n",
    "PROXY_PORT = os.getenv('PROXY_PORT', '33335')\n",
    "PROXY_USERNAME = os.getenv('PROXY_USERNAME', 'Secret...') # Get your own Username\n",
    "PROXY_PASSWORD = os.getenv('PROXY_PASSWORD', 'Secret...') # Get your Own Password\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches HTML content using Bright Data residential proxy\n",
    "    \n",
    "    Args:\n",
    "        url (str): Documentation URL to fetch\n",
    "        \n",
    "    Returns:\n",
    "        str: Raw HTML content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct proxy URL in format given below:\n",
    "        proxy_url = f\"http://{PROXY_USERNAME}:{PROXY_PASSWORD}@{PROXY_HOST}:{PROXY_PORT}\"\n",
    "        proxies = {\n",
    "            \"http\": proxy_url,\n",
    "            \"https\": proxy_url\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "        }\n",
    "        \n",
    "        # Add 30-second timeout and retry mechanism\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                # Temporarily disable SSL verification for this request\n",
    "                response = requests.get(\n",
    "                    url,\n",
    "                    headers=headers,\n",
    "                    proxies=proxies,\n",
    "                    timeout=30,\n",
    "                    verify=False  # Disable SSL verification for problematic domains\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Validate content type\n",
    "                if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "                    raise ValueError(\"URL does not return HTML content\")\n",
    "                    \n",
    "                return response.text\n",
    "                \n",
    "            except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
    "                if attempt < 2:  # Retry twice\n",
    "                    print(f\"Retrying ({attempt+1}/3) due to error: {str(e)}\")\n",
    "                    continue\n",
    "                raise\n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise ConnectionError(f\"Proxy fetch failed: {str(e)}\") from e\n",
    "\n",
    "def extract_main_content(html: str) -> tuple:\n",
    "    \"\"\"Extracts main content from MoEngage documentation HTML\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Extract title from the page\n",
    "        title_tag = soup.find('h6', class_='article-title')\n",
    "        title = title_tag.get_text().strip() if title_tag else \"Untitled\"\n",
    "        \n",
    "        # Find the main content container\n",
    "        article_body = soup.find('div', class_='article__body markdown')\n",
    "        if not article_body:\n",
    "            # Fallback to other content containers\n",
    "            article_body = soup.find('div', class_='article-body') or \\\n",
    "                          soup.find('article') or \\\n",
    "                          soup.find('main') or \\\n",
    "                          soup.body\n",
    "            \n",
    "        if not article_body:\n",
    "            raise ValueError(\"Could not identify main content area\")\n",
    "        \n",
    "        # Remove unnecessary elements\n",
    "        for element in article_body.find_all(['script', 'style', 'footer', 'nav', 'aside', 'form', 'dialog', 'div', 'ol', 'header']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Convert to Markdown\n",
    "        converter = html2text.HTML2Text()\n",
    "        converter.ignore_links = False\n",
    "        converter.ignore_images = True  # Skip images to save tokens since im using the free tier\n",
    "        converter.ignore_emphasis = False\n",
    "        converter.body_width = 0\n",
    "        \n",
    "        markdown = converter.handle(str(article_body))\n",
    "        \n",
    "        # Clean up Markdown\n",
    "        markdown = re.sub(r'\\n{3,}', '\\n\\n', markdown)\n",
    "        markdown = re.sub(r'(\\s{2,})', ' ', markdown)\n",
    "        return title, markdown.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Content extraction failed: {str(e)}\") from e\n",
    "\n",
    "def process_documentation_url(url: str) -> tuple:\n",
    "    \"\"\"Processes a documentation URL and returns title/content\"\"\"\n",
    "    html = fetch_html(url)\n",
    "    return extract_main_content(html)\n",
    "\n",
    "def save_to_file(title: str, content: str, output_dir: str = \"scraped_docs\"):\n",
    "    \"\"\"Saves content to a properly named markdown file\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create filename from title\n",
    "    filename = re.sub(r'[^\\w\\s-]', '', title).strip().lower()\n",
    "    filename = re.sub(r'[-\\s]+', '_', filename)[:50] + '.md'  # Limit filename length\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {title}\\n\\n\")\n",
    "        f.write(content)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # REPLACE THESE WITH YOUR ACTUAL BRIGHT DATA CREDENTIALS\n",
    "    os.environ['PROXY_USERNAME'] = 'Secret...' # Get your Own Username\n",
    "    os.environ['PROXY_PASSWORD'] = 'Secret...'  # Get your Own Password\n",
    "    \n",
    "    test_urls = [\n",
    "        \"https://developers.moengage.com/hc/en-us/articles/360061108111-Web-SDK-Overview#h_01H9G1YMFWVN61PKBDN0MGAJWG\",\n",
    "        \"https://developers.moengage.com/hc/en-us/articles/22105190881044-Getting-Started-with-React-Native-SDK#h_01HEJAHP5W49AASNSHF5614AHP\"\n",
    "    ]\n",
    "    \n",
    "    for url in test_urls:\n",
    "        print(f\"\\n{'='*50}\\nProcessing: {url}\\n{'='*50}\")\n",
    "        try:\n",
    "            title, content = process_documentation_url(url)\n",
    "            filepath = save_to_file(title, content)\n",
    "            print(f\"Extracted content saved to: {filepath}\")\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Content length: {len(content)} characters\")\n",
    "            print(f\"Content preview:\\n{content[:300]}...\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
