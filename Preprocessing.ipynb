{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f025ed",
   "metadata": {},
   "source": [
    "## Documentation Preprocessing\n",
    "\n",
    "### Objective\n",
    "This component prepares scraped documentation for analysis by transforming Markdown into a structured text format that enhances readability for language models. It focuses on preserving semantic structure while removing elements that could interfere with quality assessment.\n",
    "\n",
    "### Technical Approach\n",
    "#### Core Technologies\n",
    "- **Frontmatter**: For handling YAML metadata in Markdown files\n",
    "- **Python-Markdown**: For converting Markdown to HTML\n",
    "- **BeautifulSoup**: For structured HTML parsing and text extraction\n",
    "- **Regex**: For pattern-based cleaning and transformation\n",
    "\n",
    "#### Key Features\n",
    "1. **Content Cleaning**:\n",
    "   - Removes YAML frontmatter while preserving content\n",
    "   - Replaces code blocks with placeholders to maintain context\n",
    "   - Converts images to descriptive placeholders\n",
    "   - Strips formatting while preserving semantic meaning\n",
    "\n",
    "2. **Structure Preservation**:\n",
    "   - Maintains heading hierarchy with explicit level markers\n",
    "   - Processes lists with indentation to show nesting\n",
    "   - Preserves section breaks with visual separators\n",
    "   - Maintains paragraph structure with proper spacing\n",
    "\n",
    "3. **Format Optimization**:\n",
    "   - Converts bold/italic to plain text to reduce noise\n",
    "   - Removes URLs while keeping link text\n",
    "   - Eliminates HTML tags for cleaner text analysis\n",
    "   - Normalizes whitespace and line breaks\n",
    "\n",
    "### Why This Approach?\n",
    "- **Analysis Readiness**: Creates a format optimized for LLM processing while preserving document structure\n",
    "- **Noise Reduction**: Removes non-essential elements that could bias quality assessments\n",
    "- **Context Preservation**: Maintains technical context through placeholders (code blocks, images)\n",
    "- **Consistency**: Ensures uniform input format for reliable analysis results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78597e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed file saved to: preprocessed_data\\preprocessed_web_sdk_overview.md\n",
      "Preprocessing complete for: web_sdk_overview.md\n",
      "Output saved to: preprocessed_data\\preprocessed_web_sdk_overview.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import frontmatter\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_markdown(content):\n",
    "    \"\"\"Remove non-essential elements while preserving structure\"\"\"\n",
    "    # Remove YAML frontmatter\n",
    "    try:\n",
    "        post = frontmatter.loads(content)\n",
    "        content = post.content\n",
    "    except:\n",
    "        pass  # Continue if no frontmatter\n",
    "    \n",
    "    # Remove code blocks but keep placeholders\n",
    "    content = re.sub(r'```.*?```', '[CODE_BLOCK]', content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove images with alt text preservation\n",
    "    content = re.sub(r'!\\[(.*?)\\]\\(.*?\\)', r'[IMAGE: \\1]', content)\n",
    "    \n",
    "    # Convert bold/italic to plain text\n",
    "    content = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', content)\n",
    "    content = re.sub(r'\\*(.*?)\\*', r'\\1', content)\n",
    "    \n",
    "    # Preserve links but remove URLs\n",
    "    content = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', content)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    content = re.sub(r'<[^>]+>', '', content)\n",
    "    \n",
    "    # Preserve section breaks\n",
    "    content = re.sub(r'\\n{3,}', '\\n\\n[SECTION_BREAK]\\n\\n', content)\n",
    "    \n",
    "    return content.strip()\n",
    "\n",
    "def convert_to_structured_text(md_content):\n",
    "    \"\"\"Convert markdown to analysis-ready structured text\"\"\"\n",
    "    # Convert to HTML\n",
    "    html = markdown.markdown(md_content)\n",
    "    \n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Process headings with hierarchy markers\n",
    "    heading_map = {'h1': 1, 'h2': 2, 'h3': 3, 'h4': 4, 'h5': 5, 'h6': 6}\n",
    "    for tag, level in heading_map.items():\n",
    "        for heading in soup.find_all(tag):\n",
    "            heading.insert_before(f\"\\n\\nHEADING_{level}: \")\n",
    "            heading.replace_with(f\"{heading.text}\\n{'=' * (len(heading.text) + 10)}\")\n",
    "    \n",
    "    # Process lists with indentation\n",
    "    list_count = 0\n",
    "    for list_tag in soup.find_all(['ul', 'ol']):\n",
    "        list_count += 1\n",
    "        list_tag.insert_before(f\"\\n\\nLIST_START_{list_count}\")\n",
    "        for li in list_tag.find_all('li'):\n",
    "            li_text = li.get_text().strip()\n",
    "            indent = '  ' * (len(li.find_parents(['ul', 'ol'])) - 1)\n",
    "            li.replace_with(f\"\\n{indent}- {li_text}\")\n",
    "        list_tag.append(f\"\\nLIST_END_{list_count}\")\n",
    "        list_tag.unwrap()\n",
    "    \n",
    "    # Process paragraphs\n",
    "    for p in soup.find_all('p'):\n",
    "        p.insert_before(\"\\n\\n\")\n",
    "        p.append(\"\\n\")\n",
    "    \n",
    "    # Get text and clean up\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'\\[SECTION_BREAK\\]', '\\n' + '-'*50 + '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_file(input_file, output_dir):\n",
    "    \"\"\"Process single markdown file with MoEngage-specific cleaning\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    cleaned = clean_markdown(content)\n",
    "    structured_text = convert_to_structured_text(cleaned)\n",
    "    \n",
    "    # Create output filename with \"preprocessed_\" prefix\n",
    "    filename = os.path.basename(input_file)\n",
    "    output_file = os.path.join(output_dir, f\"preprocessed_{filename}\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(structured_text)\n",
    "    \n",
    "    print(f\"Preprocessed file saved to: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "INPUT_FILE = \"web_sdk_overview.md\"  \n",
    "OUTPUT_DIR = \"preprocessed_data\"  \n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_file = preprocess_file(INPUT_FILE, OUTPUT_DIR)\n",
    "    print(f\"Preprocessing complete for: {INPUT_FILE}\")\n",
    "    print(f\"Output saved to: {preprocessed_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
